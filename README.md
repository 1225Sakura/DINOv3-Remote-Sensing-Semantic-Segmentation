# DINOv3 Remote Sensing Semantic Segmentation

A comprehensive semantic segmentation framework for remote sensing imagery using DINOv3 (Vision Transformer) as the backbone. This project supports training and inference on six major remote sensing datasets.

## Features

- **Multi-Dataset Support**: Train on 6 different remote sensing datasets
- **DINOv3 Backbone**: Leverages powerful vision transformer features
- **Easy to Use**: Simple training and inference scripts
- **Production Ready**: Clean, organized codebase suitable for research and production

## Supported Datasets

| Dataset | Classes | Task | Image Size |
|---------|---------|------|------------|
| [LoveDA](https://github.com/Junjue-Wang/LoveDA) | 7 | Land-cover classification | Variable |
| [iSAID](https://captain-whu.github.io/iSAID/) | 16 | Aerial scene understanding | Variable |
| [Vaihingen](https://www.isprs.org/education/benchmarks/UrbanSemLab/) | 5 | Urban semantic labeling | Variable |
| [Potsdam](https://www.isprs.org/education/benchmarks/UrbanSemLab/) | 6 | Urban semantic labeling | Variable |
| [LandCover.ai](https://landcover.ai/) | 5 | Land cover classification | 512x512 |
| [OpenEarthMap](https://open-earth-map.org/) | 8 | Global land cover mapping | Variable |

## Project Structure

```
remote_sensing_segmentation/
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .gitignore                # Git ignore rules
â”‚
â”œâ”€â”€ datasets/                  # Dataset loaders
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loveda_dataset.py
â”‚   â”œâ”€â”€ isaid_dataset.py
â”‚   â”œâ”€â”€ vaihingen_dataset.py
â”‚   â”œâ”€â”€ potsdam_dataset.py
â”‚   â”œâ”€â”€ landcoverai_dataset.py
â”‚   â”œâ”€â”€ openearthmap_dataset.py
â”‚   â”œâ”€â”€ LoveDA/               # Dataset files (not in git)
â”‚   â”œâ”€â”€ iSAID/                # Dataset files (not in git)
â”‚   â”œâ”€â”€ Vaihingen/            # Dataset files (not in git)
â”‚   â”œâ”€â”€ Potsdam/              # Dataset files (not in git)
â”‚   â”œâ”€â”€ LandCoverai/          # Dataset files (not in git)
â”‚   â””â”€â”€ OpenEarthMap/         # Dataset files (not in git)
â”‚
â”œâ”€â”€ models/                    # Model architecture
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ segmentation_model.py
â”‚
â”œâ”€â”€ scripts/                   # Training and inference
â”‚   â”œâ”€â”€ train.py              # Main training script
â”‚   â””â”€â”€ generate_predictions.py
â”‚
â”œâ”€â”€ tests/                     # Unit tests
â”‚   â””â”€â”€ test_dataset.py
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ DATASET_SETUP.md      # Dataset setup guide
â”‚   â”œâ”€â”€ TRAINING_RESULTS.md   # Training results
â”‚   â””â”€â”€ FINAL_COMPREHENSIVE_REPORT.md
â”‚
â””â”€â”€ configs/                   # Configuration files
```

## Installation

### Prerequisites

- Python 3.8+
- CUDA 11.0+ (for GPU training)
- DINOv3 repository

### Setup

1. Clone the DINOv3 repository and this project:

```bash
# Clone DINOv3
git clone https://github.com/facebookresearch/dinov3.git
cd dinov3

# Clone this repository into dinov3/
git clone <this-repo-url> remote_sensing_segmentation
cd remote_sensing_segmentation
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Download datasets and models:

**Due to GitHub file size limitations, datasets and trained models are hosted on Baidu Cloud.**

ğŸ“¦ **Baidu Cloud Download**: https://pan.baidu.com/s/5CXLX9bODEHBSVfKVRLsmdg

The cloud storage contains:
- `model/` - Trained model files (~7.2GB) for all 6 datasets
- `æ•°æ®é›†/` - Complete datasets (optional, can also download from official sources)

For detailed download instructions and file structure, see:
- [Download Guide (English)](docs/LARGE_FILES.md)
- [ä¸‹è½½è¯´æ˜ (ä¸­æ–‡)](docs/LARGE_FILES_CN.md)

Alternatively, you can download the original datasets yourself and set them up following the [Dataset Setup Guide](docs/DATASET_SETUP.md)

## Quick Start

### Training

Train on a single dataset:

```bash
cd /path/to/dinov3
python remote_sensing_segmentation/scripts/train.py \
    --datasets loveda \
    --batch_size 4 \
    --num_workers 4
```

Train on multiple datasets:

```bash
python remote_sensing_segmentation/scripts/train.py \
    --datasets loveda isaid vaihingen \
    --batch_size 4 \
    --num_workers 4
```

Train on all datasets:

```bash
python remote_sensing_segmentation/scripts/train.py \
    --datasets all \
    --batch_size 4 \
    --num_workers 4
```

### Inference

Generate predictions for a trained model:

```bash
python remote_sensing_segmentation/scripts/generate_predictions.py \
    --datasets loveda \
    --models_dir trained_models/quick_train \
    --output_dir predictions
```

## Configuration

### Training Parameters

- `--datasets`: Which datasets to train on (`all` or specific names)
- `--batch_size`: Batch size for training (default: 4)
- `--num_workers`: Number of data loading workers (default: 4)
- `--img_size`: Image size for training (default: 512)
- `--backbone`: DINOv3 backbone variant (default: dinov3_vitl16)

### Dataset-Specific Settings

Different datasets can use different numbers of epochs. See `scripts/train.py` for configuration details.

## Model Architecture

The segmentation model uses:
- **Backbone**: DINOv3 Vision Transformer (frozen or fine-tuned)
- **Head**: Lightweight segmentation decoder
- **Loss**: Cross-entropy with optional class weights

## Results

Training results and performance metrics can be found in:
- [Training Results](docs/TRAINING_RESULTS.md)
- [Comprehensive Report](docs/FINAL_COMPREHENSIVE_REPORT.md)

## Testing

Run tests to verify dataset loading:

```bash
python tests/test_dataset.py
```

## Project Dependencies

This project depends on the DINOv3 repository for the backbone model. Make sure to:
1. Clone DINOv3 first
2. Place this project inside the DINOv3 directory
3. Follow the DINOv3 setup instructions for downloading pretrained weights

## License

This project is released under the same license as DINOv3. Please refer to the original DINOv3 repository for license details.

## Citation

If you use this code in your research, please cite:

```bibtex
@article{oquab2023dinov2,
  title={DINOv2: Learning Robust Visual Features without Supervision},
  author={Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy V. and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
  journal={arXiv preprint arXiv:2304.07193},
  year={2023}
}
```

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## Contact

For questions or issues, please open an issue on GitHub.

## Acknowledgments

- DINOv3 team at Meta AI for the excellent vision transformer backbone
- Dataset providers for making their data publicly available
- PyTorch and the open-source community
